{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FastALPR","text":""},{"location":"#intro","title":"Intro","text":"<p>FastALPR is a high-performance, customizable Automatic License Plate Recognition (ALPR) system. We offer fast and efficient ONNX models by default, but you can easily swap in your own models if needed.</p> <p>For Optical Character Recognition (OCR), we use fast-plate-ocr by default, and for license plate detection, we use open-image-models. However, you can integrate any OCR or detection model of your choice.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\udd0d High Accuracy: Uses advanced models for precise license plate detection and OCR.</li> <li>\ud83d\udd27 Customizable: Easily switch out detection and OCR models.</li> <li>\ud83d\ude80 Easy to Use: Quick setup with a simple API.</li> <li>\ud83d\udce6 Out-of-the-Box Models: Includes ready-to-use detection and OCR models</li> <li>\u26a1 Fast Performance: Optimized with ONNX Runtime for speed.</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions to the repo are greatly appreciated. Whether it's bug fixes, feature enhancements, or new models, your contributions are warmly welcomed.</p> <p>To start contributing or to begin development, you can follow these steps:</p> <ol> <li>Clone repo     <pre><code>git clone https://github.com/ankandrew/fast-alpr.git\n</code></pre></li> <li>Install all dependencies (make sure you have Poetry installed):     <pre><code>make install\n</code></pre></li> <li>To ensure your changes pass linting and tests before submitting a PR:     <pre><code>make checks\n</code></pre></li> </ol>"},{"location":"custom_models/","title":"Custom Models","text":""},{"location":"custom_models/#customization-and-flexibility","title":"\ud83d\udee0\ufe0f Customization and Flexibility","text":"<p>FastALPR is designed to be flexible. You can customize the detector and OCR models according to your requirements.</p>"},{"location":"custom_models/#using-tesseract-ocr","title":"Using Tesseract OCR","text":"<p>You can very easily integrate with Tesseract OCR to leverage its capabilities:</p> tesseract_ocr.py<pre><code>import re\nfrom statistics import mean\n\nimport numpy as np\nimport pytesseract\n\nfrom fast_alpr.alpr import ALPR, BaseOCR, OcrResult\n\n\nclass PytesseractOCR(BaseOCR):\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Init PytesseractOCR.\n        \"\"\"\n\n    def predict(self, cropped_plate: np.ndarray) -&gt; OcrResult | None:\n        if cropped_plate is None:\n            return None\n        # You can change 'eng' to the appropriate language code as needed\n        data = pytesseract.image_to_data(\n            cropped_plate,\n            lang=\"eng\",\n            config=\"--oem 3 --psm 6\",\n            output_type=pytesseract.Output.DICT,\n        )\n        plate_text = \" \".join(data[\"text\"]).strip()\n        plate_text = re.sub(r\"[^A-Za-z0-9]\", \"\", plate_text)\n        avg_confidence = mean(conf for conf in data[\"conf\"] if conf &gt; 0) / 100.0\n        return OcrResult(text=plate_text, confidence=avg_confidence)\n\n\nalpr = ALPR(detector_model=\"yolo-v9-t-384-license-plate-end2end\", ocr=PytesseractOCR())\n\nalpr_results = alpr.predict(\"assets/test_image.png\")\nprint(alpr_results)\n</code></pre> Tip <p>You can implement this with any OCR you want! For example, EasyOCR.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>For inference, install:</p> <pre><code>pip install fast-alpr[onnx-gpu]\n</code></pre> Warning <p>By default, no ONNX runtime is installed.</p> <p>To run inference, you must install one of the ONNX extras:</p> <ul> <li><code>onnx</code> - for CPU inference (cross-platform)</li> <li><code>onnx-gpu</code> - for NVIDIA GPUs (CUDA)</li> <li><code>onnx-openvino</code> - for Intel CPUs / VPUs</li> <li><code>onnx-directml</code> - for Windows devices via DirectML</li> <li><code>onnx-qnn</code> - for Qualcomm chips on mobile</li> </ul> <p>Dependencies for inference are kept minimal by default. Inference-related packages like ONNX runtimes are optional and not installed unless explicitly requested via extras.</p>"},{"location":"quick_start/","title":"Quick Start","text":""},{"location":"quick_start/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Here's how to get started with FastALPR:</p>"},{"location":"quick_start/#predictions","title":"Predictions","text":"<pre><code>from fast_alpr import ALPR\n\n# You can also initialize the ALPR with custom plate detection and OCR models.\nalpr = ALPR(\n    detector_model=\"yolo-v9-t-384-license-plate-end2end\",\n    ocr_model=\"cct-xs-v1-global-model\",\n)\n\n# The \"assets/test_image.png\" can be found in repo root dir\n# You can also pass a NumPy array containing cropped plate image\nalpr_results = alpr.predict(\"assets/test_image.png\")\nprint(alpr_results)\n</code></pre> Note <p>See reference for the available models.</p> <p>Output:</p> <p></p>"},{"location":"quick_start/#draw-results","title":"Draw Results","text":"<p>You can also draw the predictions directly on the image:</p> <pre><code>import cv2\n\nfrom fast_alpr import ALPR\n\n# Initialize the ALPR\nalpr = ALPR(\n    detector_model=\"yolo-v9-t-384-license-plate-end2end\",\n    ocr_model=\"cct-xs-v1-global-model\",\n)\n\n# Load the image\nimage_path = \"assets/test_image.png\"\nframe = cv2.imread(image_path)\n\n# Draw predictions on the image\nannotated_frame = alpr.draw_predictions(frame)\n\n# Display the result\ncv2.imshow(\"ALPR Result\", annotated_frame)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre> <p>Output:</p> <p></p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#available-models","title":"Available Models","text":"<p>See available detection models in open-image-models and OCR models in fast-plate-ocr.</p>"},{"location":"reference/#fastalpr","title":"FastALPR","text":"<p>FastALPR package.</p>"},{"location":"reference/#fast_alpr.ALPR","title":"<code>ALPR</code>","text":"<p>Automatic License Plate Recognition (ALPR) system class.</p> <p>This class combines a detector and an OCR model to recognize license plates in images.</p> Source code in <code>fast_alpr/alpr.py</code> <pre><code>class ALPR:\n    \"\"\"\n    Automatic License Plate Recognition (ALPR) system class.\n\n    This class combines a detector and an OCR model to recognize license plates in images.\n    \"\"\"\n\n    def __init__(\n        self,\n        detector: BaseDetector | None = None,\n        ocr: BaseOCR | None = None,\n        detector_model: PlateDetectorModel = \"yolo-v9-t-384-license-plate-end2end\",\n        detector_conf_thresh: float = 0.4,\n        detector_providers: Sequence[str | tuple[str, dict]] | None = None,\n        detector_sess_options: ort.SessionOptions = None,\n        ocr_model: OcrModel | None = \"cct-xs-v1-global-model\",\n        ocr_device: Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n        ocr_providers: Sequence[str | tuple[str, dict]] | None = None,\n        ocr_sess_options: ort.SessionOptions | None = None,\n        ocr_model_path: str | os.PathLike | None = None,\n        ocr_config_path: str | os.PathLike | None = None,\n        ocr_force_download: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ALPR system.\n\n        Parameters:\n            detector: An instance of BaseDetector. If None, the DefaultDetector is used.\n            ocr: An instance of BaseOCR. If None, the DefaultOCR is used.\n            detector_model: The name of the detector model or a PlateDetectorModel enum instance.\n                Defaults to \"yolo-v9-t-384-license-plate-end2end\".\n            detector_conf_thresh: Confidence threshold for the detector.\n            detector_providers: Execution providers for the detector.\n            detector_sess_options: Session options for the detector.\n            ocr_model: The name of the OCR model from the model hub. This can be none and\n                `ocr_model_path` and `ocr_config_path` parameters are expected to pass them to\n                `fast-plate-ocr` library.\n            ocr_device: The device to run the OCR model on (\"cuda\", \"cpu\", or \"auto\").\n            ocr_providers: Execution providers for the OCR. If None, the default providers are used.\n            ocr_sess_options: Session options for the OCR. If None, default session options are\n                used.\n            ocr_model_path: Custom model path for the OCR. If None, the model is downloaded from the\n                hub or cache.\n            ocr_config_path: Custom config path for the OCR. If None, the default configuration is\n                used.\n            ocr_force_download: Whether to force download the OCR model.\n        \"\"\"\n        # Initialize the detector\n        self.detector = detector or DefaultDetector(\n            model_name=detector_model,\n            conf_thresh=detector_conf_thresh,\n            providers=detector_providers,\n            sess_options=detector_sess_options,\n        )\n\n        # Initialize the OCR\n        self.ocr = ocr or DefaultOCR(\n            hub_ocr_model=ocr_model,\n            device=ocr_device,\n            providers=ocr_providers,\n            sess_options=ocr_sess_options,\n            model_path=ocr_model_path,\n            config_path=ocr_config_path,\n            force_download=ocr_force_download,\n        )\n\n    def predict(self, frame: np.ndarray | str) -&gt; list[ALPRResult]:\n        \"\"\"\n        Returns all recognized license plates from a frame.\n\n        Parameters:\n            frame: Unprocessed frame (Colors in order: BGR) or image path.\n\n        Returns:\n            A list of ALPRResult objects containing detection and OCR results.\n        \"\"\"\n        if isinstance(frame, str):\n            img_path = frame\n            frame = cv2.imread(img_path)\n            if frame is None:\n                raise ValueError(f\"Failed to load image from path: {img_path}\")\n\n        plate_detections = self.detector.predict(frame)\n        alpr_results = []\n        for detection in plate_detections:\n            bbox = detection.bounding_box\n            x1, y1 = max(bbox.x1, 0), max(bbox.y1, 0)\n            x2, y2 = min(bbox.x2, frame.shape[1]), min(bbox.y2, frame.shape[0])\n            cropped_plate = frame[y1:y2, x1:x2]\n            ocr_result = self.ocr.predict(cropped_plate)\n            alpr_result = ALPRResult(detection=detection, ocr=ocr_result)\n            alpr_results.append(alpr_result)\n        return alpr_results\n\n    def draw_predictions(self, frame: np.ndarray | str) -&gt; np.ndarray:\n        \"\"\"\n        Draws detections and OCR results on the frame.\n\n        Parameters:\n            frame: The original frame or image path.\n\n        Returns:\n            The frame with detections and OCR results drawn.\n        \"\"\"\n        # If frame is a string, assume it's an image path and load it\n        if isinstance(frame, str):\n            img_path = frame\n            frame = cv2.imread(img_path)\n            if frame is None:\n                raise ValueError(f\"Failed to load image from path: {img_path}\")\n\n        # Get ALPR results\n        alpr_results = self.predict(frame)\n\n        for result in alpr_results:\n            detection = result.detection\n            ocr_result = result.ocr\n            bbox = detection.bounding_box\n            x1, y1, x2, y2 = bbox.x1, bbox.y1, bbox.x2, bbox.y2\n            # Draw the bounding box\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (36, 255, 12), 2)\n            if ocr_result is None or not ocr_result.text or not ocr_result.confidence:\n                continue\n            # Remove padding symbols if any\n            plate_text = ocr_result.text\n            confidence: float = (\n                statistics.mean(ocr_result.confidence)\n                if isinstance(ocr_result.confidence, list)\n                else ocr_result.confidence\n            )\n            display_text = f\"{plate_text} {confidence * 100:.2f}%\"\n            font_scale = 1.25\n            # Draw black background for better readability\n            cv2.putText(\n                img=frame,\n                text=display_text,\n                org=(x1, y1 - 10),\n                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                fontScale=font_scale,\n                color=(0, 0, 0),\n                thickness=6,\n                lineType=cv2.LINE_AA,\n            )\n            # Draw white text\n            cv2.putText(\n                img=frame,\n                text=display_text,\n                org=(x1, y1 - 10),\n                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                fontScale=font_scale,\n                color=(255, 255, 255),\n                thickness=2,\n                lineType=cv2.LINE_AA,\n            )\n        return frame\n</code></pre>"},{"location":"reference/#fast_alpr.ALPR.__init__","title":"<code>__init__(detector=None, ocr=None, detector_model='yolo-v9-t-384-license-plate-end2end', detector_conf_thresh=0.4, detector_providers=None, detector_sess_options=None, ocr_model='cct-xs-v1-global-model', ocr_device='auto', ocr_providers=None, ocr_sess_options=None, ocr_model_path=None, ocr_config_path=None, ocr_force_download=False)</code>","text":"<p>Initialize the ALPR system.</p> <p>Parameters:</p> Name Type Description Default <code>detector</code> <code>BaseDetector | None</code> <p>An instance of BaseDetector. If None, the DefaultDetector is used.</p> <code>None</code> <code>ocr</code> <code>BaseOCR | None</code> <p>An instance of BaseOCR. If None, the DefaultOCR is used.</p> <code>None</code> <code>detector_model</code> <code>PlateDetectorModel</code> <p>The name of the detector model or a PlateDetectorModel enum instance. Defaults to \"yolo-v9-t-384-license-plate-end2end\".</p> <code>'yolo-v9-t-384-license-plate-end2end'</code> <code>detector_conf_thresh</code> <code>float</code> <p>Confidence threshold for the detector.</p> <code>0.4</code> <code>detector_providers</code> <code>Sequence[str | tuple[str, dict]] | None</code> <p>Execution providers for the detector.</p> <code>None</code> <code>detector_sess_options</code> <code>SessionOptions</code> <p>Session options for the detector.</p> <code>None</code> <code>ocr_model</code> <code>OcrModel | None</code> <p>The name of the OCR model from the model hub. This can be none and <code>ocr_model_path</code> and <code>ocr_config_path</code> parameters are expected to pass them to <code>fast-plate-ocr</code> library.</p> <code>'cct-xs-v1-global-model'</code> <code>ocr_device</code> <code>Literal['cuda', 'cpu', 'auto']</code> <p>The device to run the OCR model on (\"cuda\", \"cpu\", or \"auto\").</p> <code>'auto'</code> <code>ocr_providers</code> <code>Sequence[str | tuple[str, dict]] | None</code> <p>Execution providers for the OCR. If None, the default providers are used.</p> <code>None</code> <code>ocr_sess_options</code> <code>SessionOptions | None</code> <p>Session options for the OCR. If None, default session options are used.</p> <code>None</code> <code>ocr_model_path</code> <code>str | PathLike | None</code> <p>Custom model path for the OCR. If None, the model is downloaded from the hub or cache.</p> <code>None</code> <code>ocr_config_path</code> <code>str | PathLike | None</code> <p>Custom config path for the OCR. If None, the default configuration is used.</p> <code>None</code> <code>ocr_force_download</code> <code>bool</code> <p>Whether to force download the OCR model.</p> <code>False</code> Source code in <code>fast_alpr/alpr.py</code> <pre><code>def __init__(\n    self,\n    detector: BaseDetector | None = None,\n    ocr: BaseOCR | None = None,\n    detector_model: PlateDetectorModel = \"yolo-v9-t-384-license-plate-end2end\",\n    detector_conf_thresh: float = 0.4,\n    detector_providers: Sequence[str | tuple[str, dict]] | None = None,\n    detector_sess_options: ort.SessionOptions = None,\n    ocr_model: OcrModel | None = \"cct-xs-v1-global-model\",\n    ocr_device: Literal[\"cuda\", \"cpu\", \"auto\"] = \"auto\",\n    ocr_providers: Sequence[str | tuple[str, dict]] | None = None,\n    ocr_sess_options: ort.SessionOptions | None = None,\n    ocr_model_path: str | os.PathLike | None = None,\n    ocr_config_path: str | os.PathLike | None = None,\n    ocr_force_download: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the ALPR system.\n\n    Parameters:\n        detector: An instance of BaseDetector. If None, the DefaultDetector is used.\n        ocr: An instance of BaseOCR. If None, the DefaultOCR is used.\n        detector_model: The name of the detector model or a PlateDetectorModel enum instance.\n            Defaults to \"yolo-v9-t-384-license-plate-end2end\".\n        detector_conf_thresh: Confidence threshold for the detector.\n        detector_providers: Execution providers for the detector.\n        detector_sess_options: Session options for the detector.\n        ocr_model: The name of the OCR model from the model hub. This can be none and\n            `ocr_model_path` and `ocr_config_path` parameters are expected to pass them to\n            `fast-plate-ocr` library.\n        ocr_device: The device to run the OCR model on (\"cuda\", \"cpu\", or \"auto\").\n        ocr_providers: Execution providers for the OCR. If None, the default providers are used.\n        ocr_sess_options: Session options for the OCR. If None, default session options are\n            used.\n        ocr_model_path: Custom model path for the OCR. If None, the model is downloaded from the\n            hub or cache.\n        ocr_config_path: Custom config path for the OCR. If None, the default configuration is\n            used.\n        ocr_force_download: Whether to force download the OCR model.\n    \"\"\"\n    # Initialize the detector\n    self.detector = detector or DefaultDetector(\n        model_name=detector_model,\n        conf_thresh=detector_conf_thresh,\n        providers=detector_providers,\n        sess_options=detector_sess_options,\n    )\n\n    # Initialize the OCR\n    self.ocr = ocr or DefaultOCR(\n        hub_ocr_model=ocr_model,\n        device=ocr_device,\n        providers=ocr_providers,\n        sess_options=ocr_sess_options,\n        model_path=ocr_model_path,\n        config_path=ocr_config_path,\n        force_download=ocr_force_download,\n    )\n</code></pre>"},{"location":"reference/#fast_alpr.ALPR.draw_predictions","title":"<code>draw_predictions(frame)</code>","text":"<p>Draws detections and OCR results on the frame.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>ndarray | str</code> <p>The original frame or image path.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The frame with detections and OCR results drawn.</p> Source code in <code>fast_alpr/alpr.py</code> <pre><code>def draw_predictions(self, frame: np.ndarray | str) -&gt; np.ndarray:\n    \"\"\"\n    Draws detections and OCR results on the frame.\n\n    Parameters:\n        frame: The original frame or image path.\n\n    Returns:\n        The frame with detections and OCR results drawn.\n    \"\"\"\n    # If frame is a string, assume it's an image path and load it\n    if isinstance(frame, str):\n        img_path = frame\n        frame = cv2.imread(img_path)\n        if frame is None:\n            raise ValueError(f\"Failed to load image from path: {img_path}\")\n\n    # Get ALPR results\n    alpr_results = self.predict(frame)\n\n    for result in alpr_results:\n        detection = result.detection\n        ocr_result = result.ocr\n        bbox = detection.bounding_box\n        x1, y1, x2, y2 = bbox.x1, bbox.y1, bbox.x2, bbox.y2\n        # Draw the bounding box\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (36, 255, 12), 2)\n        if ocr_result is None or not ocr_result.text or not ocr_result.confidence:\n            continue\n        # Remove padding symbols if any\n        plate_text = ocr_result.text\n        confidence: float = (\n            statistics.mean(ocr_result.confidence)\n            if isinstance(ocr_result.confidence, list)\n            else ocr_result.confidence\n        )\n        display_text = f\"{plate_text} {confidence * 100:.2f}%\"\n        font_scale = 1.25\n        # Draw black background for better readability\n        cv2.putText(\n            img=frame,\n            text=display_text,\n            org=(x1, y1 - 10),\n            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n            fontScale=font_scale,\n            color=(0, 0, 0),\n            thickness=6,\n            lineType=cv2.LINE_AA,\n        )\n        # Draw white text\n        cv2.putText(\n            img=frame,\n            text=display_text,\n            org=(x1, y1 - 10),\n            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n            fontScale=font_scale,\n            color=(255, 255, 255),\n            thickness=2,\n            lineType=cv2.LINE_AA,\n        )\n    return frame\n</code></pre>"},{"location":"reference/#fast_alpr.ALPR.predict","title":"<code>predict(frame)</code>","text":"<p>Returns all recognized license plates from a frame.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>ndarray | str</code> <p>Unprocessed frame (Colors in order: BGR) or image path.</p> required <p>Returns:</p> Type Description <code>list[ALPRResult]</code> <p>A list of ALPRResult objects containing detection and OCR results.</p> Source code in <code>fast_alpr/alpr.py</code> <pre><code>def predict(self, frame: np.ndarray | str) -&gt; list[ALPRResult]:\n    \"\"\"\n    Returns all recognized license plates from a frame.\n\n    Parameters:\n        frame: Unprocessed frame (Colors in order: BGR) or image path.\n\n    Returns:\n        A list of ALPRResult objects containing detection and OCR results.\n    \"\"\"\n    if isinstance(frame, str):\n        img_path = frame\n        frame = cv2.imread(img_path)\n        if frame is None:\n            raise ValueError(f\"Failed to load image from path: {img_path}\")\n\n    plate_detections = self.detector.predict(frame)\n    alpr_results = []\n    for detection in plate_detections:\n        bbox = detection.bounding_box\n        x1, y1 = max(bbox.x1, 0), max(bbox.y1, 0)\n        x2, y2 = min(bbox.x2, frame.shape[1]), min(bbox.y2, frame.shape[0])\n        cropped_plate = frame[y1:y2, x1:x2]\n        ocr_result = self.ocr.predict(cropped_plate)\n        alpr_result = ALPRResult(detection=detection, ocr=ocr_result)\n        alpr_results.append(alpr_result)\n    return alpr_results\n</code></pre>"},{"location":"reference/#fast_alpr.ALPRResult","title":"<code>ALPRResult</code>  <code>dataclass</code>","text":"<p>Dataclass to hold the results of detection and OCR for a license plate.</p> Source code in <code>fast_alpr/alpr.py</code> <pre><code>@dataclass(frozen=True)\nclass ALPRResult:\n    \"\"\"\n    Dataclass to hold the results of detection and OCR for a license plate.\n    \"\"\"\n\n    detection: DetectionResult\n    ocr: OcrResult | None\n</code></pre>"},{"location":"reference/#fast_alpr.BaseDetector","title":"<code>BaseDetector</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>fast_alpr/base.py</code> <pre><code>class BaseDetector(ABC):\n    @abstractmethod\n    def predict(self, frame: np.ndarray) -&gt; list[DetectionResult]:\n        \"\"\"Perform detection on the input frame and return a list of detections.\"\"\"\n</code></pre>"},{"location":"reference/#fast_alpr.BaseDetector.predict","title":"<code>predict(frame)</code>  <code>abstractmethod</code>","text":"<p>Perform detection on the input frame and return a list of detections.</p> Source code in <code>fast_alpr/base.py</code> <pre><code>@abstractmethod\ndef predict(self, frame: np.ndarray) -&gt; list[DetectionResult]:\n    \"\"\"Perform detection on the input frame and return a list of detections.\"\"\"\n</code></pre>"},{"location":"reference/#fast_alpr.BaseOCR","title":"<code>BaseOCR</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>fast_alpr/base.py</code> <pre><code>class BaseOCR(ABC):\n    @abstractmethod\n    def predict(self, cropped_plate: np.ndarray) -&gt; OcrResult | None:\n        \"\"\"Perform OCR on the cropped plate image and return the recognized text and character\n        probabilities.\"\"\"\n</code></pre>"},{"location":"reference/#fast_alpr.BaseOCR.predict","title":"<code>predict(cropped_plate)</code>  <code>abstractmethod</code>","text":"<p>Perform OCR on the cropped plate image and return the recognized text and character probabilities.</p> Source code in <code>fast_alpr/base.py</code> <pre><code>@abstractmethod\ndef predict(self, cropped_plate: np.ndarray) -&gt; OcrResult | None:\n    \"\"\"Perform OCR on the cropped plate image and return the recognized text and character\n    probabilities.\"\"\"\n</code></pre>"}]}